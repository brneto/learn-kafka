= Kafka Streams Documentation
Berchris Requiao <brneto@gmail.com>
:doctype: book
:icons: font
:source-highlighter: highlightjs
:toc: left
:toclevels: 4
:tabsize: 4
:sectlinks:

:pkgsrc: ../../main/java/com/zinkworks/streams
:smtconcept: https://docs.confluent.io/platform/current/connect/concepts.html
:devguide: https://kafka.apache.org/32/documentation/streams/developer-guide/processor-api.html#defining-a-stream-processor
:mtopic: https://gist.github.com/andrewlouis93/5fd10d8041aeaf733d3acfbd61f6bbef
:topicstragety: https://medium.com/streamthoughts/understanding-kafka-partition-assignment-strategies-and-how-to-write-your-own-custom-assignor-ebeda1fc06f3
:rangeassignor: https://kafka.apache.org/32/javadoc/org/apache/kafka/clients/consumer/RangeAssignor.html

[[abstract]]
This document provides an overview of the logic used by the _Kafka Streams_ existing in the _com.zinkworks.streams_ package.
[TIP]
_Kafka Streams_ are part of the _Kafka_ API.
More info can be found on {devguide}[Kafka Developer Guide].

[[storeBuilder]]
== Create Global Store Builder
Here it is defined the Global Store Builder:

====
.StoreBuilder Store Builder
[source,java,indent=0]
....
include::{pkgsrc}/ProcessorApi.java[tag=globalStoreBuilder]
....
<1> Create the store not making use of the changelog topic
====

== Add a Global Store
Here it is added a Global Store to the stream topology:

====
.AddStore Add Store
[source,java,indent=0]
....
include::{pkgsrc}/ProcessorApi.java[tag=globalStore]
....
<1> Like a source node, global store is considered a type of node and therefore needs to get its own name
====
NOTE: Because all the topic record are copied to the global store it is not allowed have the same topic registered to a global store and as a source node in the same topology object. In such case Kafka will throw a `TopologyException` exception error.

Although Global State Store add no constraints on the number of streams instances using it regardless the number of partitions existing in the topic it reads from, the number of stream instances in kafka is always limited by the number of partitions of the source node in the topology the processor is in.

== Example of Kafka Stream DSL
Here it can be seen an example of how to create a kafka stream using the DSL API,
instead of the Processor API:

====
.kafkaDSL Kafka Stream DSL
[source,java,indent=0]
....
include::{pkgsrc}/ProcessorApi.java[tag=kstream]
....
====

== Define Punctuation Schedule
The Processor API developers can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents their customized processing logic.footnote:[{devguide}[Kafka Stream Developer Guide.]]

The process method is performed on each of the received record, but those processed records are only forwarded to the downstream processors based on the punctuation scheduled:

====
.schedule Forward Processed Records
[source,java,indent=0]
....
include::{pkgsrc}/processor/TotalPriceOrderProcessor.java[tag=schedulePunctuation]
....
====

== Kafka Topic In Memory Load via Global Store Updater vs Stream Processor Terminal Operation
Regardless you use _Global Store Updater_ or _Stream Processor Terminal Operation_ you will always need to be concerned about co-partitioning. In either cases you only able to scale the Enrichment modules increasing the partitions number from the *Source Topic*. The real difference between them is how the topics partitions are assigned to each instance.

For Global Store Updater all that matters is how many partitions the *Source Input* have because the Global Store Updater will always read all records from the *Reference Topic* regardless the number of partitions there. While the _Stream Processor Terminal Operation_ will be assigned based on the partition assigment strategyfootnote:[{topicstragety}[Understanding Kafka partition assignment strategies and how to write your own custom assignor.]] configured for the topics. The default strategy is the `RangeAssignor`footnote:[{rangeassignor}[RangeAssignor (kafka 3.2.0 API).]] where it divides the number of partitions over the number of consumer instances, and assign one by one. If the division does not result in a nice round number, the first few consumer instances are assigned more partitions.footnote:[{mtopic}[Consuming from multiple topics.]]

The only risk we might face using the _Stream Processor Terminal Operation_ is if the number of partitions in the *Reference* and *Source* topics are not the same. Due to how Kafka assign records partitions based on record's key _hashcode_, we could end up having records from the same key being consumed by different instances. This means that to be able to scale up the instances safely it needs to be guaranteed that the *Reference Topic* and *Source Topic* has always the same number of partitions. This can be enforced via code by throwing an exception in case this requirement is not obeyed, in similar manner as done by the _KTables_ for full join operations.
