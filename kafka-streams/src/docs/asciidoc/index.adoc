= Kafka Streams Documentation
Berchris Requiao <brneto@gmail.com>
:doctype: book
:icons: font
:source-highlighter: highlightjs
:toc: left
:toclevels: 4
:tabsize: 4
:sectlinks:

:pkgsrc: ../../main/java/com/zinkworks/streams
:smtconcept: https://docs.confluent.io/platform/current/connect/concepts.html
:devguide: https://kafka.apache.org/32/documentation/streams/developer-guide/processor-api.html#defining-a-stream-processor

[[abstract]]
This document provides an overview of the logic used by the _Kafka Streams_ existing in the _com.zinkworks.streams_ package.
[TIP]
_Kafka Streams_ are part of the _Kafka_ API.
More info can be found on {devguide}[Kafka Developer Guide].

[[storeBuilder]]
== Create Global Store Builder
Here it is defined the Global Store Builder:

====
.StoreBuilder Store Builder
[source,java,indent=0]
....
include::{pkgsrc}/ProcessorApi.java[tag=globalStoreBuilder]
....
<1> Create the store not making use of the changelog topic
====

== Add a Global Store
Here it is added a Global Store to the stream topology:

====
.AddStore Add Store
[source,java,indent=0]
....
include::{pkgsrc}/ProcessorApi.java[tag=globalStore]
....
<1> Like a source node, global store is considered a type of node and therefore needs to get its own name
====
NOTE: Because all the topic record are copied to the global store it is not allowed have the same topic registered to a global store and as a source node in the same topology object. In such case Kafka will throw a `TopologyException` exception error.

Although Global State Store add no constraints on the number of streams instances using it regardless the number of partitions existing in the topic it reads from, the number of stream instances in kafka is always limited by the number of partitions of the source node in the topology the processor is in.

== Example of Kafka Stream DSL
Here it can be seen an example of how to create a kafka stream using the DSL API,
instead of the Processor API:

====
.kafkaDSL Kafka Stream DSL
[source,java,indent=0]
....
include::{pkgsrc}/ProcessorApi.java[tag=kstream]
....
====

== Define Punctuation Schedule
The Processor API developers can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents their customized processing logic.footnote:[{devguide}[Kafka Stream Developer Guide].]

The process method is performed on each of the received record, but those processed records are only forwarded to the downstream processors based on the punctuation scheduled:

====
.schedule Forward Processed Records
[source,java,indent=0]
....
include::{pkgsrc}/processor/TotalPriceOrderProcessor.java[tag=schedulePunctuation]
....
====

== Kafka Topic In Memory Load
When the reference topic has only one partition and the source topic has multiple partitions, the in memory data structure behaves in similar fashion as a global store. Thus, all stream processor instances will share the same data and this data structure will contain all the record of the reference topic.

The number of stream processor instances are solely determined by the number of partitions existing in the source topic. This is true regardless the system is using a global store updater or a stream processor terminal operation to keep track of the reference topic in the in memory data structure. What actually happens when multiple instance of the stream processor reading from source topic is created is that the same number of instances of the stream processor reading from reference topic is also created but since the reference topic has only one partitions the surplus instances are left there doing nothing.

In case reference topic has more partitions than the source topic, the number of instance of the stream processor reading from the reference topic are again restricted by the number of partitions in the source topic.

The conclusion is in both cases (using the global store update or the stream processor terminal operation) the in memory data structure will always be sharded based on the number of indepedent instances running which are defined by the number of partitions in source topic being read by the stream processor. When increase in scalability is required the action need is only increase in the same amount the number of partitions in the source topic and in the reference topic.